# -*- coding: utf-8 -*-
"""cluster_analysis-ila-03.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MoJ55JLRjJMox9kLcq5hDO5hpv5t56PY

#Document Clustering with Python

In this guide, I will explain how to cluster a set of documents using Python. My motivating example is to identify the latent structures within the synopses of the top 100 films of all time (per an IMDB list). See [the original post](http://www.brandonrose.org/top100) for a more detailed discussion on the example. This guide covers:

<ul>
<li> tokenizing and stemming each synopsis
<li> transforming the corpus into vector space using [tf-idf](http://en.wikipedia.org/wiki/Tf%E2%80%93idf)
<li> calculating cosine distance between each document as a measure of similarity
<li> clustering the documents using the [k-means algorithm](http://en.wikipedia.org/wiki/K-means_clustering)
<li> using [multidimensional scaling](http://en.wikipedia.org/wiki/Multidimensional_scaling) to reduce dimensionality within the corpus
<li> plotting the clustering output using [matplotlib](http://matplotlib.org/) and [mpld3](http://mpld3.github.io/)
<li> conducting a hierarchical clustering on the corpus using [Ward clustering](http://en.wikipedia.org/wiki/Ward%27s_method)
<li> plotting a Ward dendrogram
<li> topic modeling using [Latent Dirichlet Allocation (LDA)](http://en.wikipedia.org/wiki/Latent_Dirichlet_allocation)
</ul>

##Contents

#PROCESS 1:
<ul>
<li>[Stopwords, stemming, and tokenization](#Stopwords,-stemming,-and-tokenizing)
</ul>

#PROCESS 2:
<ul>
<li>[Tf-idf and document similarity](#Tf-idf-and-document-similarity)
</ul>

#PROCESS 3:
<ul>
<li>[K-means clustering](#K-means-clustering)
</ul>

#PROCESS 4:
<ul>
<li>[Preparing data to tables](#Organize data in tables)
<li>[Multidimensional scaling](#Multidimensional-scaling)
<li>[Visualizing document clusters](#Visualizing-document-clusters)
</ul>

#PROCESS 5:
<ul>
<li>[Hierarchical document clustering](#Hierarchical-document-clustering)
<li>[Latent Dirichlet Allocation (LDA)](#Latent-Dirichlet-Allocation)
</ul>

But first, I import everything I am going to need up front
"""
from __future__ import print_function

import numpy as np
import pandas as pd
import nltk
nltk.download('stopwords')
nltk.download('punkt')
import re
import os
import pickle
import codecs
from sklearn import feature_extraction
#!pip install mpld3
import mpld3
import json,sys

params = sys.argv[1]
params = json.loads(params)


"""# PROCESS 3

#K-means clustering

Now onto the fun part. Using the tf-idf matrix, you can run a slew of clustering algorithms to better understand the hidden structure within the synopses. I first chose [k-means](http://en.wikipedia.org/wiki/K-means_clustering). K-means initializes with a pre-determined number of clusters (I chose 5). Each observation is assigned to a cluster (cluster assignment) so as to minimize the within cluster sum of squares. Next, the mean of the clustered observations is calculated and used as the new cluster centroid. Then, observations are reassigned to clusters and  centroids recalculated in an iterative process until the algorithm reaches convergence.

I found it took several runs for the algorithm to converge a global optimum as k-means is susceptible to reaching local optima.
"""

# Commented out IPython magic to ensure Python compatibility.
from sklearn.cluster import KMeans

km = KMeans(n_clusters=params['num_clusters'])

tfidf_matrix = pickle.load(open(params['representation_filename'], "rb" ))

# %time km.fit(tfidf_matrix)
km.fit(tfidf_matrix)

clusters = km.labels_.tolist()

#from sklearn.externals import joblib
import joblib

joblib.dump(km,  params['model_output'])
#km = joblib.load(params['model_filename'])
#clusters = km.labels_.tolist()


#pickle.dump(clusters, open(params['clusters_output_filename'], "wb" ))


#titles = pickle.load(open(params['titles_filename'], "rb" ))
#topics = pickle.load(open(params['topics_filename'], "rb" )) 
#print(topics)
#ranks = pickle.load( open( "09-ranks.pkl", "rb" ) )
#totalTitles = len(titles)
#texts = pickle.load(open(params['content_filename'], "rb" ))

#terms = pickle.load(open(params['terms_filename'], "rb" ))

import pandas as pd
#dict_frame = {'title': titles, 'rank': ranks, 'text': texts, 'cluster': clusters, 'genre': topics}
#frame = pd.DataFrame(dict_frame, index = [clusters] , columns = ['rank', 'title', 'cluster', 'genre'])

#print(frame.head())

frame = pd.read_csv(params['input_frame_filename'])
ranks = [i for i in range(frame.shape[0])]
frame.index = clusters  # can be a list, a Series, an array or a scalar   
frame.insert(loc=0, column='rank', value=ranks)
frame.insert(loc=2, column='cluster', value=clusters)
#frame.drop('synopsis', axis=1, inplace=True)
frame['cluster'].value_counts()
grouped = frame['rank'].groupby(frame['cluster'])
grouped.mean()

#from __future__ import print_function



'''
print("Top terms per cluster:")
print()
vocab_frame = pickle.load( open( "02-vocab_frame.pkl", "rb" ) )
order_centroids = km.cluster_centers_.argsort()[:, ::-1]
for i in range(params['num_clusters']):
    print("Cluster %d words:" % i, end='')
    for ind in order_centroids[i, :6]:
        print(' %s' % vocab_frame.loc[terms[ind].split(' ')].values.tolist()[0][0].encode('utf-8', 'ignore'), end=',')
    print()
    print()
    print("Cluster %d titles:" % i, end='')
    for title in frame.loc[i]['title'].values.tolist():
        print(' %s,' % title, end='')
    print()
    print()
'''


#This is purely to help export tables to html and to correct for my 0 start rank (so that Godfather is 1, not 0)
frame['Rank'] = frame['rank'] + 1
frame['Title'] = frame['title']

print(frame.head())


#pickle.dump(frame, open(params['output_frame_filename'], "wb" ))
frame.to_csv(params['frame_output'], index=False)
